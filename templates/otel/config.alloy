// Listen to all interfaces to help with dockers sending data
// Note that we don't open the firewall for external hosts
otelcol.receiver.otlp "otel" {
  grpc {
    endpoint = "[::]:4317"
  }
  http {
    endpoint = "[::]:4318"
  }
  output {
    metrics = [otelcol.processor.batch.otel.input]
    logs    = [otelcol.processor.batch.otel.input]
    traces  = [otelcol.processor.batch.otel.input]
  }
}
// Batch send eveything to the default exporter below. 
otelcol.processor.batch "otel" {
  output {
    metrics = [otelcol.exporter.otlp.default.input]
    logs    = [otelcol.processor.transform.fixes.input]
    traces  = [otelcol.exporter.otlp.default.input]
  }
}
loki.relabel "journal" {
  forward_to = []

  rule {
    source_labels = ["__journal__systemd_unit"]
    target_label  = "service_name" // This is the new otel name for a service instead of the ad-hoc unit
  }
  rule {
    source_labels = ["__journal__hostname"]
    target_label  = "hostname"
  }
  rule {
    source_labels = ["__journal__boot_id"]
    target_label  = "boot_id"
  }
  rule {
    source_labels = ["__journal__machine_id"]
    target_label  = "machine_id"
  }
  rule {
    source_labels = ["__journal__priority"]
    target_label  = "level"
  }
  rule {
    source_labels = ["__journal__syslog_identifier"]
    target_label  = "syslog_identifier"
  }
  rule {
    source_labels = ["__journal__transport"]
    target_label  = "transport"
  }
}
// Map old style loki names to opentelemetry resource.attributes
// https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/7298#issuecomment-3187693464
otelcol.processor.transform "fixes" {
  error_mode = "silent"

  log_statements {
    context = "log"
    statements = [
      `set(resource.attributes["service.name"], attributes["service_name"])`,
      `set(resource.attributes["host.name"], attributes["hostname"])`,
      `set(resource.attributes["host.id"], attributes["machine_id"])`,
      `set(attributes["severityname"], attributes["level"])`,
      `delete_key(attributes, "hostname")`,
      `delete_key(attributes, "loki_attribute_labels")`,
      `delete_key(attributes, "machine_id")`,
    ]
  }

  output {
    logs    = [otelcol.exporter.otlp.default.input]
  }
}

otelcol.receiver.loki "default" {
  output {
    metrics = [otelcol.processor.batch.otel.input]
    logs    = [otelcol.processor.batch.otel.input]
    traces  = [otelcol.processor.batch.otel.input]
  }
}
loki.source.journal "read"  {
  forward_to    = [otelcol.receiver.loki.default.receiver]
  relabel_rules = loki.relabel.journal.rules
  labels        = {component = "loki.source.journal"}
}

// Export local metrics, work the same as the older prometheus-node-exporter
prometheus.exporter.unix "default" {
  include_exporter_metrics = true
  enable_collectors = ["systemd", "processes"]
}
// Listen to prometheus
otelcol.receiver.prometheus "default" {
  output {
    metrics = [otelcol.exporter.otlp.default.input]
  }
}
// change name of job to node
discovery.relabel "node_exporter" { 
    // workaround that the job name get set to 'integrations/unix' instead of 'node' that the dashboard expects
  targets = prometheus.exporter.unix.default.targets
  rule {
    action       = "replace"
    target_label = "job"
    replacement  = "node"
  }
}
discovery.file "targetsd" {
  files = ["/etc/alloy/targets.d/*.yaml"]
}
// scrape our own metrics 
prometheus.scrape "default" {
  targets = discovery.relabel.node_exporter.output
  forward_to = [
    otelcol.receiver.prometheus.default.receiver,
  ]
}
prometheus.scrape "targetsd" {
  targets = discovery.file.targetsd.targets
  forward_to = [
    otelcol.receiver.prometheus.default.receiver,
  ]
}
// Write everything to the receiver below. 
otelcol.exporter.otlp "default" {
  client {
    endpoint = "<%= @otel_receiver %>:4317"
  }
}
