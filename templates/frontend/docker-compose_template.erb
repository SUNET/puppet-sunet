---
version: '3.4'

services:

  haproxy:
    image: '<%= @haproxy_image %>:<%= @haproxy_imagetag %>'
    expose:
      - 80
      - 443
<% if @varnish_enabled == true -%>
      - 1080
<%- end -%>
<% if @extra_ports.is_a? Array -%>
<% @extra_ports.each do |port| -%>
      - "<%= port %>"
<%- end -%>
<%- end -%>
    # No point in restarting the haproxy container - the systemd service ExecStartPost will not
    # execute again, so the restarted haproxy container won't get the sarimner0 interface and
    # nothing will work anyways until the whole service is restarted
    restart: 'no'
    volumes:
      - '/opt/frontend/scripts/haproxy-start.sh:/haproxy-start.sh:ro'
      - 'haproxy_control:/haproxy_control'
      - 'haproxy_new_config:/new_config:ro'
      - 'haproxy_running_config:/etc/haproxy'
<% if @tls_certificate_bundle -%>
      - <%= @tls_certificate_bundle_on_host -%>:<%= @tls_certificate_bundle_in_container -%>:ro
<% else -%>
      # tls_certificate_bundle not set in Puppet
<% end -%>
<% if @haproxy_volumes.is_a? Array -%>
      # haproxy_volumes :
<% @haproxy_volumes.each do |this| -%>
      - '<%= this %>'
<% end -%>
<% end -%>
<% if @dns.is_a? Array -%>
    dns:
<% @dns.each do |server| -%>
      - '<%= server %>'
<% end -%>
    sysctls:
<% end -%>
    depends_on:
      - config
      - monitor
    environment:
      - 'WAIT_FOR_INTERFACE=sarimner0'
<% if @varnish_enabled == true -%>
      - 'WAIT_FOR_CONTAINER=varnish'
<% end -%>
      - 'HAPROXY_RUNDIR=/dev/shm'  # needs to be writable by the haproxy user
      - 'HAPROXY_NEW_CONFIG=/new_config/haproxy.cfg'
    command: /haproxy-start.sh
    user: "<%= scope['sunet::frontend::load_balancer::users::user2uid']['haproxy'] -%>:<%= scope['sunet::frontend::load_balancer::users::user2uid']['haproxy'] -%>"

<% if @varnish_enabled == true -%>
  varnish:
    image: '<%= @varnish_image %>:<%= @varnish_imagetag %>'
    expose:
      - "1080"
    volumes:
      - <%= @varnish_config %>:/etc/varnish/default.vcl:ro
    tmpfs:
      - '/var/lib/varnish:uid=<%= scope['sunet::frontend::load_balancer::users::user2uid']['varnish'] -%>,gid=<%= scope['sunet::frontend::load_balancer::users::user2uid']['varnish'] -%>,exec'
    command: varnishd -F -f /etc/varnish/default.vcl -s <%= @varnish_storage %> -a 0.0.0.0:1080
    restart: always
    depends_on:
      - haproxy
<% if @dns.is_a? Array -%>
    dns:
<% @dns.each do |server| -%>
      - '<%= server %>'
<% end -%>
<% end -%>
    user: "<%= scope['sunet::frontend::load_balancer::users::user2uid']['varnish'] -%>:<%= scope['sunet::frontend::load_balancer::users::user2uid']['varnish'] -%>"
<% else -%>
  # varnish disabled
<% end -%>


  config:
    image: '<%= @frontendtools_image %>:<%= @frontendtools_imagetag %>'
    restart: always
    depends_on:
      - setup_volumes
    volumes:
<% if @frontendtools_volumes.is_a? Array -%>
      # frontendtools_volumes :
<% @frontendtools_volumes.each do |this| -%>
      - '<%= this %>'
<% end -%>
<% end -%>
      - /opt/frontend/config/common:/opt/frontend/config/common:ro
      - /opt/frontend/config/<%= @instance %>:/opt/frontend/config/<%= @instance %>:ro
      - /opt/frontend/api/backends/<%= @site_name %>:/opt/frontend/api/backends/<%= @site_name %>:ro
      - haproxy_new_config:/new_config:rw
    environment:
      - 'HAPROXY_NEW_CONFIG=/new_config/haproxy.cfg'
    command: /opt/frontend/scripts/generate-haproxy-config --instance <%= @instance %> --haproxy_template <%= @instance %>/haproxy.j2
    user: "<%= scope['sunet::frontend::load_balancer::users::user2uid']['fe-config'] -%>:<%= scope['sunet::frontend::load_balancer::users::user2uid']['fe-config'] -%>"


  monitor:
    image: '<%= @frontendtools_image %>:<%= @frontendtools_imagetag %>'
    restart: always
    depends_on:
      - setup_volumes
    volumes:
<% if @frontendtools_volumes.is_a? Array -%>
      # frontendtools_volumes :
<% @frontendtools_volumes.each do |this| -%>
      - '<%= this %>'
<% end -%>
<% end -%>
      - /opt/frontend/config/common:/opt/frontend/config/common:ro
      - /opt/frontend/config/<%= @instance %>/config.yml:/opt/frontend/config/<%= @instance %>/config.yml:ro
      - /opt/frontend/monitor/<%= @instance %>:/opt/frontend/monitor/<%= @instance %>:rw
      - haproxy_control:/haproxy_control
    command: >
        /opt/frontend/scripts/monitor-haproxy
            <% if @statsd_enabled == true -%>--statsd_host <%= @statsd_host %> --statsd_prefix sarimner.<%= @instance %><% end -%>
            --stats_url "/haproxy_control/stats"
            'site=<%= @site_name %>;group=<%= @monitor_group %>'
<% if @dns.is_a? Array -%>
    dns:
<% @dns.each do |server| -%>
      - '<%= server %>'
<% end -%>
<% end -%>
    environment:
      - 'HOSTFQDN=<%= @fqdn %>'
      - 'INSTANCE=<%= @instance %>'
      - 'SITENAME=<%= @site_name %>'
      - 'STATUSFN=/dev/shm/haproxy-status'  # need to be writable by user fe-monitor, and match the healthcheck
      - 'STATSSOCKET=/haproxy_control/stats'  # need to match --stats_url in command
    healthcheck:
      test: 'head -1 /dev/shm/haproxy-status | grep -q ^UP'
      interval: 2s
      start_period: 1m
    # need group fe-config to read configuration to calculate announce parameters XXX TODO NOT POSSIBLE
    user: "<%= scope['sunet::frontend::load_balancer::users::user2uid']['fe-monitor'] -%>:<%= scope['sunet::frontend::load_balancer::users::user2uid']['fe-monitor'] -%>"


  # This just needs to run once as root to allow the other containers to have user: something
  setup_volumes:
    image: 'docker.sunet.se/frontend/frontend-tools:<%= @frontendtools_imagetag %>'
    labels:
      - "se.sunet.check_docker_containers=runonce"
    volumes:
      - /etc/passwd:/etc/passwd:ro
      - /etc/group:/etc/group:ro
      - 'haproxy_control:/haproxy_control:rw'
      - 'haproxy_new_config:/haproxy_new_config:rw'
      - 'haproxy_running_config:/haproxy_running_config:rw'
    # The fe-config user writes new config to this directory, and the haproxy user reads it
    entrypoint: >
       /bin/bash -c '
         id;
         export D="/haproxy_control"; chown -R haproxy:fe-monitor "$${D}"; chmod 2750 "$${D}"; find "$${D}" -ls;
         export D="/haproxy_new_config"; chown -R fe-config:haproxy "$${D}"; chmod 2750 "$${D}"; find "$${D}" -ls;
         export D="/haproxy_running_config"; chown -R haproxy:haproxy "$${D}"; chmod 700 "$${D}"; find "$${D}" -ls;
         '
    # In case a container with a set user gets committed to this image...
    user: 'root:root'

volumes:
  haproxy_control:
  haproxy_new_config:
  haproxy_running_config:

# Provide user-friendly name of bridge interface
networks:
  default:
    driver_opts:
      com.docker.network.bridge.name: br-<%= @instance %>
