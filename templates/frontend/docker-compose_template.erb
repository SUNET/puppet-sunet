---
version: '3.4'

services:

  haproxy:
    image: '<%= @haproxy_image %>:<%= @haproxy_imagetag %>'
    expose:
      - "443"
<% if @extra_ports.is_a? Array -%>
<% @extra_ports.each do |port| -%>
      - "<%= port %>"
<%- end -%>
<%- end -%>
    # No point in restarting the haproxy container - the systemd service ExecStartPost will not
    # execute again, so the restarted haproxy container won't get the sarimner0 interface and
    # nothing will work anyways until the whole service is restarted
    restart: 'no'
    volumes:
      - '/opt/frontend/scripts/haproxy-start.sh:/haproxy-start.sh:ro'
      - 'haproxy_data:/etc/haproxy'
      - 'haproxy_control:/var/run/haproxy-control'
<% if @tls_certificate_bundle -%>
      - /opt/frontend/config/<%= @instance %>/certs/tls_certificate_bundle.pem:<%= @tls_certificate_bundle %>:ro
<% else -%>
      # tls_certificate_bundle not set in Puppet
<% end -%>
<% if @haproxy_volumes.is_a? Array -%>
      # haproxy_volumes :
<% @haproxy_volumes.each do |this| -%>
      - '<%= this %>'
<% end -%>
<% end -%>
<% if @dns.is_a? Array -%>
    dns:
<% @dns.each do |server| -%>
      - '<%= server %>'
<% end -%>
    sysctls:
<% end -%>
    depends_on:
      - setup_data
      - config
      - monitor
    environment:
      - 'WAIT_FOR_INTERFACE=sarimner0'
<% if @varnish_enabled == true -%>
      - 'WAIT_FOR_CONTAINER=varnish'
<% end -%>
      - 'HAPROXYMASTERSOCK=/dev/shm/haproxy-master.sock'  # needs to be writable by the haproxy user
    command: /haproxy-start.sh
    user: "<%= scope['sunet::frontend::load_balancer::users::user2uid']['haproxy'] -%>:<%= scope['sunet::frontend::load_balancer::users::user2uid']['haproxy'] -%>"

<% if @varnish_enabled == true -%>
  varnish:
    image: '<%= @varnish_image %>:<%= @varnish_imagetag %>'
    expose:
      - "1080"
    volumes:
      - <%= @varnish_config %>:/etc/varnish/default.vcl:ro
    command: varnishd -F -f /etc/varnish/default.vcl -s <%= @varnish_storage %> -a 0.0.0.0:1080
    restart: always
    depends_on:
      - haproxy
<% if @dns.is_a? Array -%>
    dns:
<% @dns.each do |server| -%>
      - '<%= server %>'
<% end -%>
<% end -%>
<% else -%>
  # varnish disabled
<% end -%>
    user: "<%= scope['sunet::frontend::load_balancer::users::user2uid']['varnish'] -%>:<%= scope['sunet::frontend::load_balancer::users::user2uid']['varnish'] -%>"


  config:
    image: 'docker.sunet.se/frontend/frontend-tools:<%= @frontendtools_imagetag %>'
    restart: always
    volumes:
<% if @frontendtools_volumes.is_a? Array -%>
      # frontendtools_volumes :
<% @frontendtools_volumes.each do |this| -%>
      - '<%= this %>'
<% end -%>
<% end -%>
      - /opt/frontend/config/common:/opt/frontend/config/common:ro
      - /opt/frontend/config/<%= @instance %>:/opt/frontend/config/<%= @instance %>:ro
      - /opt/frontend/api/backends/<%= @site_name %>:/opt/frontend/api/backends/<%= @site_name %>:ro
      - haproxy_data:/etc/haproxy
    command: /opt/frontend/scripts/generate-haproxy-config --instance <%= @instance %> --haproxy_template <%= @instance %>/haproxy.j2
    user: "<%= scope['sunet::frontend::load_balancer::users::user2uid']['fe-config'] -%>:<%= scope['sunet::frontend::load_balancer::users::user2uid']['fe-config'] -%>"


  monitor:
    image: 'docker.sunet.se/frontend/frontend-tools:<%= @frontendtools_imagetag %>'
    restart: always
    volumes:
<% if @frontendtools_volumes.is_a? Array -%>
      # frontendtools_volumes :
<% @frontendtools_volumes.each do |this| -%>
      - '<%= this %>'
<% end -%>
<% end -%>
      - /opt/frontend/config/common:/opt/frontend/config/common:ro
      - /opt/frontend/config/<%= @instance %>:/opt/frontend/config/<%= @instance %>:ro
      - /opt/frontend/monitor/<%= @instance %>:/opt/frontend/monitor/<%= @instance %>:rw
      - haproxy_control:/var/run/haproxy-control
    command: >
        /opt/frontend/scripts/monitor-haproxy
            <% if @statsd_enabled == true -%>--statsd_host <%= @statsd_host %> --statsd_prefix sarimner.<%= @instance %><% end -%>
            --stats_url /var/run/haproxy-control/stats
            'site=<%= @site_name %>;group=<%= @monitor_group %>'
<% if @dns.is_a? Array -%>
    dns:
<% @dns.each do |server| -%>
      - '<%= server %>'
<% end -%>
<% end -%>
    environment:
      - 'HOSTFQDN=<%= @fqdn %>'
      - 'INSTANCE=<%= @instance %>'
      - 'SITENAME=<%= @site_name %>'
      - 'STATUSFN=/dev/shm/haproxy-status'  # need to be writable by user fe-monitor
    healthcheck:
      test: 'head -1 /dev/shm/haproxy-status | grep -q ^UP'
      interval: 2s
      start_period: 1m
    user: "<%= scope['sunet::frontend::load_balancer::users::user2uid']['fe-monitor'] -%>:<%= scope['sunet::frontend::load_balancer::users::user2uid']['fe-monitor'] -%>"


  # This just needs to run once as root to allow the other containers to have user: something
  setup_data:
    image: 'docker.sunet.se/frontend/frontend-tools:<%= @frontendtools_imagetag %>'
    volumes:
      - /etc/passwd:/etc/passwd:ro
      - /etc/group:/etc/group:ro
      - 'haproxy_data:/haproxy_data'
    entrypoint: "sh -c 'chown -R haproxy:fe-config /haproxy_data; chmod 770 /haproxy_data'"


  # This just needs to run once as root to allow the other containers to have user: something
  setup_control:
    image: 'docker.sunet.se/frontend/frontend-tools:<%= @frontendtools_imagetag %>'
    volumes:
      - /etc/passwd:/etc/passwd:ro
      - /etc/group:/etc/group:ro
      - 'haproxy_control:/haproxy_control'
    entrypoint: "sh -c 'chown -R haproxy: /haproxy_control'"


volumes:
  haproxy_data:
  haproxy_control:

# Provide user-friendly name of bridge interface
networks:
  default:
    driver_opts:
      com.docker.network.bridge.name: br-<%= @instance %>
